{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37d694bc-963d-41b3-a113-5f2fcf740029",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h1>Part A – Application area review</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bbc0a7-8b7d-4f0d-8d1f-e3e9c2213b75",
   "metadata": {},
   "source": [
    "<h2>Image Colorization</h2>\n",
    "<h3>Domain Overview</h3>\n",
    "<p>\n",
    "Colorization is the process of converting black and white images to colored images. Typical the process is conducted manually via image editing applications such as Adobe photoshop. Layers upon Layers of color is carefully painted in by digital artists and can span upwards of 100 layers per image. This pain staking process can take a tremendous amount of labor as well as time. Recently with the advancement of computer vision utilizing Deep Learning technologies research on to Automatic colorization has become an active research domain. The problem is scoped as using Artificial Intelligence to convert a grey scale image to a colored image.  The application of this technology ranges from restoration of old photographs to the field of medical imaging.\n",
    "</p>\n",
    "<p>\n",
    "<h3>Movie Colorization</h3>\n",
    "One of the domains in which colorization is actively used is within the Film industry. The earliest research into colorization was conducted by Author Markle (Markle, 1984) which used the aid of computer technologies for coloring old movies. A computer tracking system was implemented track the luminance values and unknown pixels were colored by calculating the surrounding pixel hue. Recent Approaches utilize deep learning to train a neural network model to automatically map between black and white images to colored images. Historical Footage of old black and white movies can be brought back to reality with the addition of color. The example below showcases an image (Left hand side) shot in 1936 by photographer Dorothea Lange. On the Right-hand side is the colorized version of the image using Deoldify, an open-source Colorization software. Such Colorization techniques were employed by film director Peter Jackson on his 2018 documentary “they shall not grow old” which depicted real word war 1 footage which were colorized and restored with the help of artificial intelligence.\n",
    "</p>\n",
    "<img src=\"Images/Deoldfy.jpg\" width=\"500\" alt=\"deoldfy example\"/>\n",
    "<em><a href =\"https://github.com/jantic/DeOldify\">Figure 1: DeOldify Example </a></em>\n",
    "\n",
    "<h3>Sketch Colorization</h3>\n",
    "<p>\n",
    "   Colorization can be extended to colorize line sketched Black and White mangas a form of japanese comics. This Technique uses pattern continuity as well as intensity continuity to add color to sketched images. The intensity continuity is used to break colors into boundaries such that it does not bleed into/ overlap other areas (Qu, 2006). Such sketch colorization techniques have been expanded with the Advancements of Generative Adversarial networks as showcased by Auto-painter  ( Luo, et al., 2018) in which Users can indicate their preferred colors and the system would generate a synthesized image which is colorized based on the user input.\n",
    "</p>\n",
    "  <img src=\"Images/sketchColorization.png\" width=\"400\" alt=\"Manga Colorization\"/>\n",
    "<em><a href =\"http://www.cse.cuhk.edu.hk/~ttwong/papers/manga/manga.pdf\">Figure 2: Manga Colorization Process </a></em>\n",
    "\n",
    "<h3>Astronomical Image Colorization</h3>\n",
    "<p>\n",
    "    Research conducted by Author Shreyas Kalvankar and Author Gao Xian Peh Applied Image Colorization to astronomical photography data (Kalvankar, et al., 2021). Data stored in the Hubble Legacy Archives is usually stored in grayscale footage and is not suitable for sharing with the public nor aid astronomers in identification. Since mannually processing such data would take a tremendous amount of efffort With the utilization of GANs These images can be colorized as well as upscaled in order to be used for various purposes.\n",
    "</p>\n",
    "\n",
    "<h3>Electron Colorization</h3>\n",
    "<p>\n",
    "    Electron Microscopes are used within the field of science to visualize structures which are at the nano/microscale level, Unlike traditional microscopes with utilize light in order to reflect back the image, Microscopes such as SEM (scanning Electron Microscope) and AFM(Atomic Force Microscope) use beams of electrons to bounce back the image which causes the image to be in grayscale. The singular channel image cannot fully visualize the proper image containing the microscopic structure. Due to this Author Israel Goytom showcases the advantages of using an automatic image colorization model. (Goytom, et al., 2019)\n",
    "    \n",
    "</p>\n",
    "<h3>Medical Image Colorization</h3>\n",
    "<p>\n",
    " In the Medical Domain Scan such as CT, MRI, PET is typically produced in a greyscale format, The addition of color to such gray scale images increases the readability such that a better and timely diagnostic can be given to the patient. Author Selvapriya showcases the effectiveness of using Selected Color masking in order to get a better and more informed diagnostics(Muhammad Usman Ghani Khan, et al., 2017)\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b5c6e3-2292-461a-802e-f9bc301417ac",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h1>Part B – Compare and evaluate AI techniques</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdcf28ab-6917-4194-b80e-4f1a6c90bdc8",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h3>1. AutoEncoders </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3430f816-7dc8-4658-8941-847eb2749cc8",
   "metadata": {},
   "source": [
    "Autoencoders learn feature representation via the process of dimensionality reduction. The architecture consists of an encoder, a bottle neck and finally a decoder. In the Encoder the input in compressed presentation and within the bottleneck the feature presentation is captured. Finally in the decoder the proceed input is \"decompressed\" using the feature presentation knowledge via the bottleneck. Due to the bottleneck only the most vital parts of feature representation are sent to the decoder.\n",
    "\n",
    "\n",
    "<table width=\"100%\" border='10'>\n",
    "    <tr>\n",
    "        <th width=\"25%\">Strengths</th>\n",
    "        <td>\n",
    "             <ul>\n",
    "                <li>Reduction of Dimensionality of data</li>\n",
    "                <li>High level of feature extraction</li>\n",
    "                <li>Improve Training Time</li>\n",
    "            </ul>  \n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>Weakness</th>\n",
    "        <td> \n",
    "            <ul>\n",
    "                <li> High computational Time</li>\n",
    "                <li>Need significant amount of data</li>\n",
    "                <li>Focuses on capturing more data rather than the most relevant</li>\n",
    "            </ul> \n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>Advantages</th>\n",
    "        <td>  \n",
    "            <ul>\n",
    "                <li> Can learn high level feature representations</li>\n",
    "            </ul>\n",
    "        </td>\n",
    "    </tr>\n",
    "     <tr>\n",
    "        <th>Disadvantages</th>\n",
    "        <td>\n",
    "          <ul>\n",
    "                <li> The computational Time is quite high</li>\n",
    "                <li>Features can be ignored during the training process</li>\n",
    "            </ul> \n",
    "        </td>\n",
    "    </tr>\n",
    "     <tr>\n",
    "        <th>Example</th>\n",
    "        <td>As shown by the research paper Deep Kolorization(Baldassarre, 2017) Autoencoders can be used for image colorization. This architecture was choosen for  implementation in this coursework.Due to the pre-trained model using such a architecture allows us to visualize the results even after a couple of hundred epochs of training </td>\n",
    "    </tr>\n",
    "    \n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95779081-6127-4b92-85c3-e3e89e61b920",
   "metadata": {},
   "source": [
    "<h3>2. Conditional Genarative Adverserial Networks</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a265e5-8dad-4e14-836f-adaf0cf00edd",
   "metadata": {},
   "source": [
    "As shown by the research paper \"Image-to-Image Translation with Conditional Adversarial Networks\" since colorization can be considered image to image translation, A conditional GAN can used to colorize gray scale images. A typical GAN contains two models, The discriminator as well as the generator. The goal of the generator is to fool the discriminator and the goal of the discriminator is to identify the difference between a fake image and a real image. These two modules are training adversarially. In a C-GAN a condition is feed into both the discriminator as well as the generator. The condition for Image colorization would be the grayscale image.\n",
    "\n",
    "<table width=\"100%\" border='10'>\n",
    "    <tr>\n",
    "        <th width=\"25%\">Strengths</th>\n",
    "        <td>\n",
    "             <ul>\n",
    "                <li>Can be trained in an unsupervised method</li>\n",
    "                <li>Can be used to generate realistic data</li>\n",
    "                <li>Can learn density distribution of data</li>\n",
    "            </ul>  \n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>Weakness</th>\n",
    "        <td> \n",
    "            <ul>\n",
    "                <li>Generator can start generating the same Image (Mode collapse) </li>\n",
    "                <li>Vanishing Gradient significant amount of data</li>\n",
    "                <li>Focuses on capturing more data rather than the most relevant</li>\n",
    "            </ul> \n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>Advantages</th>\n",
    "        <td>  \n",
    "            <ul>\n",
    "                <li>Can train a model for colorization with a smaller dataset</li>\n",
    "            </ul>\n",
    "        </td>\n",
    "    </tr>\n",
    "     <tr>\n",
    "        <th>Disadvantages</th>\n",
    "        <td>\n",
    "          <ul>\n",
    "                <li> Computationally complex for limited hardware resources</li>\n",
    "            </ul> \n",
    "        </td>\n",
    "    </tr>\n",
    "     <tr>\n",
    "        <th>Example</th>\n",
    "        <td>Both the generator and discriminator are conditioned on the grayscale image with the generator attempted to create a colorized output, this output is fed into the discriminator which tries to differentiate between the input and a real image. An implementation of the architecture can be found on the \"Image-to-Image Translation with Conditional Adversarial Networks\" paper which uses a Pix2Pix Model</td>\n",
    "    </tr>\n",
    "    \n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2badc0-5533-448e-a7d5-06ce2fd01f44",
   "metadata": {},
   "source": [
    "<h3> 3. Support Vector Machines</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dceb0dd-6c66-4783-b268-63963809a45b",
   "metadata": {},
   "source": [
    "\n",
    "<table width=\"100%\" border='10'>\n",
    "    <tr>\n",
    "        <th width=\"25%\">Strengths</th>\n",
    "        <td>\n",
    "             <ul>\n",
    "                <li>Memory Efficient</li>\n",
    "                <li>Good when the dimentionality is high</li>\n",
    "            </ul>  \n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>Weakness</th>\n",
    "        <td> \n",
    "            <ul>\n",
    "                <li>Not suitable for large datasets</li>\n",
    "                <li>Doesn't perform well if the data is noisy</li>\n",
    "            </ul> \n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>Advantages</th>\n",
    "        <td>  \n",
    "            <ul>\n",
    "                <li>Accurate in high dimentional Space</li>\n",
    "            </ul>\n",
    "        </td>\n",
    "    </tr>\n",
    "     <tr>\n",
    "        <th>Disadvantages</th>\n",
    "        <td>\n",
    "          <ul>\n",
    "                <li> image colorization requires a large dataset</li>\n",
    "            </ul> \n",
    "        </td>\n",
    "    </tr>\n",
    "     <tr>\n",
    "        <th>Example</th>\n",
    "        <td>As shown by the research paper \"Machine Learning Methods for Automatic\n",
    "Image Colorization\" Colorization is considered as an prediction problem and color variations are learned using a method known as \"Parzen window\"\n",
    "</td>\n",
    "    </tr>\n",
    "    \n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d307606a-f860-48da-8e10-eb1fbdde9168",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h1>Part C - Implementation</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e261d7eb-c561-49ab-83b9-bfc24f38c062",
   "metadata": {},
   "source": [
    "<h3>Overview</h3>\n",
    "</p>This section contains the details of the implementation selected, After doing a brief literature review as well as asking guidance from experts regarding the computational efforts which would be required, Image Colorization using the Autoencoder technique was chosen as the implementation. Due to the size of the model as well as the large dataset used, training on a local machine would have not been productive hence the decision was taken to train using <a href=\"https://colab.research.google.com/?utm_source=scs-index\"> Google's Colaboratory</a>, a cloud based jupyter notebook environment which can be used to train machine learning models. Training was done using the GPU instants of Colab. The preprocessing/training process in Colab can be seen the following notebook:\n",
    "</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bad4d56-937e-4622-ac93-212c1b6b3f49",
   "metadata": {},
   "source": [
    "[Training & testing details](./Catography_fusion_final.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c1bc96-4804-47f6-ad13-f623251c9175",
   "metadata": {},
   "source": [
    "<h3>Input</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba74ce20-9c65-474d-a11b-87b1c0ba4cf2",
   "metadata": {},
   "source": [
    "<p>Link to dataset : <a href=\"https://www.kaggle.com/tongpython/cat-and-dog\">Cats and Dogs</a></p>\n",
    "<p>The model requires gray scaled versions of images as well as mapping to the respective colorized image. The choice of dataset can vastly affect the outcome of the model as diverse datasets can result in the models output resulting in brownish images (details in the results section). For this coursework I browsed through the Kaggle website looking through image data. A few pre-requisites which I had gathering during my research was that the model performed better on similar looking images as well as images had to be reshaped to the size of at minimum 256 x 256 due to computational limitation of training a large-scale model such as this. After careful consideration I ended up choosing the Cat and Dog Dataset which contained 10,000 images of cats and dogs, split in to 8,000 files for training and 2,000 files for testing purposes. The Dataset was imported onto google Colab using the Kaggle API and store in google drive for ease of access. Due to the complexity and time constraints only the cat images were chosen as the training data.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f188ac0-397a-4aeb-a5f0-4b619560d311",
   "metadata": {},
   "source": [
    "<h3>Preprocessing</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c936ae5-6cf2-4f01-b978-70593dabbe46",
   "metadata": {},
   "source": [
    "<p>As mentioned in the above sections in order to train a network to colorize grayscale images, The model is given grayscale images which is then transformed into a colorized one. Images generally are represented in the standard RGB color space. A single pixel will contain values for each of one of the red, green and blue channels ranging between 0-255, This results in 16 million color space per pixel. While a single pixel in a grayscale image contains one channel ranging from 0-255, which is a 256 color space per pixel. while this mapping is possible due to the large range of values to be predicted, research have deduced using a different color space such as LAB would result in a better output.\n",
    "</p>\n",
    "<b>LAB Color Space</b>\n",
    "<p>\n",
    "    <ul>\n",
    "        <li> L = Luminance/Lightness</li>\n",
    "        <li> A = Red/Green Channel</li>\n",
    "        <li> B = Blue/Yellow Channel</li>\n",
    "    </ul>\n",
    "The LAB color space gives a more accurate and detailed enriched Image which can be processed. To formulate the input for this model the L channel can be perceived as a grayscale’ d image while the A and B channels are taken as the models output. The mapping in converted from the original 1 color channel to 3 color channel output to a more sensible 1 color channel to 2 color channel mapping. This significantly reduces the search space from 16 million combinations to around 65,000.\n",
    "</p>\n",
    "<p>\n",
    "To normalize the values initially each RGB image is brough down to a scale in the range of 0-1. The value range for both A and B channels within the LAB color space ranges from -127 to 128, hence the values are brought down to a range of 0-1 as well. Due to the limitation of computational resources all 8000 images could not be loaded into the model at once. A custom generator using Kera's ImageDataGenerator and Flow_from_Directory functions are utilized to feed the model in batches. Details of the implementation can be found in the training Details Notebook.\n",
    "</p>\n",
    "<p>Each RGB Image is shaped to 255 x 255 pixels </p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9475e9-322f-4d06-bf48-3997db7d1e3d",
   "metadata": {},
   "source": [
    "<h3>Model</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9ec7a1-2cbb-45f0-84d5-388e93c7905c",
   "metadata": {},
   "source": [
    "<p>\n",
    "The implemented architecture was originally proposed by researcher Federico Baldassarre on their paper <a href=\"https://arxiv.org/abs/1712.03400\">Deep Koalarization: Image Colorization using CNNs and Inception-ResNet-v2</a><br/>\n",
    "The model is made up on an auto encoder architecture which is trained in parallel with an Inception v2 pretrained model for feature extraction. The standard bottleneck of an autoencoder is replaced by a fusion layer to concatenate the features from both the encoder as well as Inception model. The output of the fusion layer is feed into decoder network which decompresses the input with all features in the fusion layer and outputs an image with A and B channels\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5578ff-0b42-4136-8ce4-76fefd102a6c",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h3>High Level Diagram</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a7baa6-6147-46c0-a27c-cb81c06aa02b",
   "metadata": {},
   "source": [
    "<img src=\"images/model.png\"/>\n",
    "<em>Figure 3: Model Overview(self composed)</em>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce567c03-2c5b-4bb9-9506-b9ebf8c1fb17",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h3>Model Training Details</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afa203a-0bc3-4f4c-8656-29524cfae565",
   "metadata": {},
   "source": [
    "<p>\n",
    "The preprocessing/training process was done in Colab can be seen the following notebook :\n",
    "</p>\n",
    "\n",
    "[Training & training details](./Catography_fusion_final.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208de9bc-1522-47fc-bf95-b4b06445fec5",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h1>Part E- Results Discussion </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f1c96a-cd0d-4cec-b9da-188288e68f21",
   "metadata": {},
   "source": [
    "The above notebook was training using Google Colab, On the notebook [Training & training details](./Catography_fusion_final.ipynb) code details as well as the results of the model is shown.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4c837f-55b4-4878-ac26-aabff425229d",
   "metadata": {},
   "source": [
    "Typically, Image colorization requires a vast amount of data as well as computational resources to effectively convert a greyscale image into a colorized version. Various research papers state Atleast upwards of 20,000 images for training and Atleast 1000 epochs for the model to reach an acceptable status. Due to time constraints as well as lack of proper computational infrastructure, the implementation was done using 4000 images of cats and trained for 218 epochs with each epoch taking around 3-4 mins to complete execution. A Data generator was implemented to feed the model since all 4000 images could not be loaded into the memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d15983d-e54e-4c12-9126-c0e3fec0039e",
   "metadata": {},
   "source": [
    "<p>Since the free use nature of Google colab each runtime was limited to around 6 hours of computational time followed by a cooldown period which reached around 12 hours at times which caused the model training to extend to a couple of days. The model was evaluated after each Runtime disconnection and the following results showcases how the model improves over time. </p>\n",
    "<img src=\"images/progress.png\">\n",
    "<em>Figure 4: Model progress(self composed)</em>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579680fb-f70e-46a4-85e6-5e22a215db14",
   "metadata": {},
   "source": [
    "<p>As shown from above the model improves the colorization process as the number of epochs increase. Ideally the model would perform on par with at least 500-1000 epochs of training but doe the time constraints as well as the computational limitation the model was stopped at around 220 epochs. After analyzing the results of the final model a few deductions were made:</p>\n",
    "<ol>\n",
    "    <li>Model requires at least around 1000 epochs of training </li>\n",
    "    <li>While Cats within the image are accurately colored, the model has a hard time learning coloring the background accurately, this could be a result of the images usually having a white or non-colorful background. A change of dataset could provide better results </li>\n",
    "    <li>Using a different architecture such as a pix2pix architecture would improve the colorization progress</li>\n",
    "    <li>Images of a smaller scale would improve computational speed but would reduce the useability of the application for general purpose colorization </li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de05c161-30f2-4682-bd15-753ee22c60c8",
   "metadata": {},
   "source": [
    "<h1>References</h1>\n",
    "Luo, . Z., Liu, Y., Qin, Z. & Wang, H., 2018. Auto-painter: Cartoon image generation from sketch by using conditional generative adversarial networks. Neurocomputing, Volume 311, pp. 78-87.<br/>\n",
    "Goytom, I. et al., 2019. Nanoscale Microscopy Images Colourisation Using Neural Networks. ArXiv..<br/>\n",
    "Kalvankar, S., Pandit, H., Parwate, P. & Patil, A., 2021. ASTRONOMICAL IMAGE COLORIZATION AND UPSCALING WITH GENERATIVE ADVERSARIAL NETWORKS. arxiv..<br/>\n",
    "Markle, W., 1984. The Development and Application of Colorization. SMPTE, Volume vol. 93, pp. 632-635..<br/>\n",
    "Muhammad Usman Ghani Khan, Nida, N. & Gotoh, Y., 2017. Medical Image Colorization for Better Visualization and Segmentation. MIUA 2017: Medical Image Understanding and Analysis, pp. 571-580..<br/>\n",
    "Qu, Y., 2006. Manga colorization. ACM Transactions on Graphics, 25(13), p. 1214–1220..<br/>\n",
    "Baldassarre, F., 2017. Deep Koalarization: Image Colorization using CNNs and Inception-Resnet-v2. arXiv<br/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a162e03-ffb5-40b3-8a79-fc7b00a0f593",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
